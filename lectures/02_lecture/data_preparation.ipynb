{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation for ML\n",
    "\n",
    "Before training any ML model, you will have to invest some time in data preparation. There are usually two steps:\n",
    "\n",
    "- Data Cleaning is very general and usually needs to be done before training. This step should be applied iteratively during EDA.\n",
    "- Data Transformations are usually very ML task/algorithm specific. The goal is to convert data into a suitable form for the task/algorithm.\n",
    "\n",
    "> ✏️ The example is inspired by {cite}`packtpublishing`, {cite}`scikit_preprocessing` and {cite}`scikit_imputation`.\n",
    "\n",
    "Let's start by importing packages:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "\n",
    "This section will show you some techniques for dealing with noisy datasets. In general, there are two approaches:\n",
    "\n",
    "- Throwing noisy instances away\n",
    "- Applying some techniques to data to reduce the noise\n",
    "\n",
    "Usually, throwing data away is not the best option because acquiring data (and additional samples) is very expensive, so we try to salvage as much data as possible. Of course, it should only be done to some extent because, in many cases, some samples are beyond recovery).\n",
    "\n",
    "### Handling Duplications\n",
    "\n",
    "In many cases, data handed to us might contain duplicated instances (for example, a single measurement is duplicated several times). Having such data might introduce a lot of unwanted biases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handling Duplications\n",
    "\n",
    "In many cases, data handed to us might contain duplicated instances (for example, a single measurement is duplicated several times). Having such data might introduce a lot of unwanted biases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handling missing or incorrect data\n",
    "\n",
    "It is very often that we receive a noisy dataset with some incorrect or missing measurements. There are many ways how to approach the problem.\n",
    "\n",
    "#### Dropping instances or attributes\n",
    "\n",
    "As discussed earlier, removing information should be done cautiously; let's review some examples:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Replacing values\n",
    "\n",
    "This is the easiest option if you have any domain knowledge of how wrong or missing values should be treated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Filling in missing data\n",
    "\n",
    "Let's start with some basics techniques:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are also more sophisticated options:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Transformations\n",
    "\n",
    "Let's review some techniques that should help you to transform data into a form that is required by the ML task/algorithm.\n",
    "\n",
    "#### Discretization\n",
    "\n",
    "Discretization (known as quantization or binning) allows the partitioning of continuous features into discrete values. Specific datasets may benefit from discretization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Encoding Variables\n",
    "\n",
    "Often features are not given as continuous values but as categorical. ML algorithms do not always support such representation. In that case, we might need to apply different encodings:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Standardization, or mean removal, variance scaling, and non-linear transformations\n",
    "\n",
    "Many ML algorithms might have some requirements for input distributions of features. It might also be beneficial to transform a feature distribution as it can significantly improve convergence speed or lead to better performance. Let's review some techniques:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercises\n",
    "\n",
    "#### Try to add some new features as a combination of the available ones to a dataset. Do a simple EDA to analyze patterns.\n",
    "\n",
    "Often it’s helpful to add complexity to a model by considering nonlinear features of the input data. Review implementation of [polynomial features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) and apply it to the Titanic dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "titanic_url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
    "titanic_data = pd.read_csv(titanic_url)\n",
    "\n",
    "# TODO: your answer here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resources\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
